{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision.models as models\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches, patheffects\n",
    "import pandas as pd\n",
    "from PIL import ImageDraw, ImageFont\n",
    "from collections import namedtuple, OrderedDict\n",
    "from session import *\n",
    "from LR_Schedule.cos_anneal import CosAnneal\n",
    "from LR_Schedule.cyclical import Cyclical\n",
    "from LR_Schedule.lr_find import lr_find\n",
    "from callbacks import *\n",
    "from validation import *\n",
    "from Vision.ImageHelpers import *\n",
    "from Vision.SSD import *\n",
    "from Datasets.RoadDamage import RoadDamageDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0); torch.backends.cudnn.benchmark=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bg', 'D00', 'D01', 'D10', 'D11', 'D20', 'D40', 'D43', 'D44']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imsize = 224\n",
    "batch_size = 2\n",
    "data, classes, train_tfms, val_tfms, denorm = RoadDamageDataset('C:/fastai/courses/dl2/data/road_damage_dataset', imsize, batch_size)\n",
    "num_classes = len(classes) - 1\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StdConv(nn.Module):\n",
    "    def __init__(self, n_in, n_out, stride=2, drop_p=0.1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(n_in, n_out, kernel_size=3, stride=stride, padding=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.batch_norm = nn.BatchNorm2d(n_out)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.batch_norm(self.relu(self.conv(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_conv(x,k=1):\n",
    "    bs,nf,gx,gy = x.size()\n",
    "    x = x.permute(0,2,3,1).contiguous()\n",
    "    return x.view(bs,-1,nf//k)\n",
    "\n",
    "class SSDOut(nn.Module):\n",
    "    def __init__(self, n_in, k=1):\n",
    "        super().__init__()\n",
    "        self.out_classes = nn.Conv2d(n_in, (num_classes + 1) * k, 3, padding=1) # Output for each class + background class\n",
    "        self.out_boxes = nn.Conv2d(n_in, 4*k, 3, padding=1) # Output for bounding boxes\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return [flatten_conv(self.out_classes(x), k), F.tanh(flatten_conv(self.out_boxes(x), k))] \n",
    "\n",
    "class SSDHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.conv_0 = StdConv(512, 256, stride=1)\n",
    "        self.conv_1 = StdConv(256, 256)\n",
    "        self.out = SSDOut(256)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(x))\n",
    "        x = self.conv_0(x)\n",
    "        x = self.conv_1(x)\n",
    "        return self.out(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_colr = 12\n",
    "cmap = get_cmap(num_colr)\n",
    "colr_list = [cmap(float(x)) for x in range(num_colr)]\n",
    "\n",
    "def show_ground_truth(ax, x, bbox, clas=None, prs=None, thresh=0.3, show_bg=False):\n",
    "    im = np.moveaxis(x, 0, 2)\n",
    "    bb = [center_to_hw(o) for o in bbox.reshape(-1,4)]\n",
    "    if prs is None:  prs  = [None]*len(bb)\n",
    "    if clas is None: clas = [None]*len(bb)\n",
    "    ax = show_img(im.clip(0,1), ax=ax)\n",
    "    for i,(b,c,pr) in enumerate(zip(bb, clas, prs)):\n",
    "        if((b[2]>0) and (pr is None or pr > thresh) and (show_bg or c != 0)):\n",
    "            draw_rect(ax, b, color=colr_list[i%num_colr])\n",
    "            txt = f'{i}: '\n",
    "            if c is not None: txt += classes[c]\n",
    "            if pr is not None: txt += f' {pr:.2f}'\n",
    "            draw_text(ax, b[:2], txt, color=colr_list[i%num_colr])\n",
    "            \n",
    "def torch_gt(ax, ima, bbox, clas, prs=None, thresh=0.25, show_bg=False):\n",
    "    return show_ground_truth(ax, ima, (bbox*imsize),\n",
    "         clas, prs if prs is not None else None, thresh, show_bg=show_bg)\n",
    "\n",
    "def test(sess, anchors, grid_sizes, data):\n",
    "    with EvalModel(sess.model):\n",
    "        rawx, rawy, *_ = next(iter(data))\n",
    "        pred_classes, bb_outputs = sess.forward(rawx)\n",
    "        prints = {key: val.numpy() for key, val in rawy.items()}\n",
    "        y = {key: Variable(value) for key, value in rawy.items()}\n",
    "\n",
    "        for i, x in enumerate(rawx[0:16]):\n",
    "            im = denorm(rawx[i]).numpy()\n",
    "\n",
    "            pred_classes_1, bb_outputs_1 = pred_classes[i], bb_outputs[i]\n",
    "            label_bbs, label_classes = y['BB'][i], y['CAT'][i]\n",
    "\n",
    "            fig, axes = plt.subplots(3, 2, figsize=(18, 18))\n",
    "\n",
    "\n",
    "            # Ground Truth\n",
    "            show_ground_truth(axes.flat[0], im, prints['BB'][i], prints['CAT'][i])\n",
    "\n",
    "\n",
    "            # Anchorbox Assignments \n",
    "            gt_bbs, gt_classes, *_ = map_label_to_ground_truth(label_bbs, label_classes, anchors, imsize)\n",
    "            torch_gt(axes.flat[1], im, anchors.cpu().data.numpy(), gt_classes.cpu().data.numpy(), show_bg=False)\n",
    "\n",
    "\n",
    "            # Predicted classes per anchorbox\n",
    "            torch_gt(axes.flat[2], im, \n",
    "                     anchors.cpu().data.numpy(), \n",
    "                     pred_classes_1.max(1)[1].data, \n",
    "                     pred_classes_1.max(1)[0].sigmoid().data, \n",
    "                     show_bg=True)\n",
    "\n",
    "\n",
    "            # Predicted classes per anchorbox. No background\n",
    "            torch_gt(axes.flat[3], im, \n",
    "                     anchors.cpu().data.numpy(), \n",
    "                     pred_classes_1[:,1:].max(1)[1].data + 1, \n",
    "                     pred_classes_1[:,1:].max(1)[0].sigmoid().data, \n",
    "                     thresh=0.15,\n",
    "                     show_bg=False)\n",
    "\n",
    "\n",
    "            # Predicted class and bounding box\n",
    "            a_ic = map_bb_outputs_to_pred_bbs(bb_outputs_1, anchors, grid_sizes)\n",
    "            torch_gt(axes.flat[4], im, \n",
    "                     a_ic.cpu().data.numpy(), \n",
    "                     pred_classes_1[:,1:].max(1)[1].data + 1, \n",
    "                     pred_classes_1[:,1:].max(1)[0].sigmoid().data, \n",
    "                     thresh=0.15, \n",
    "                     show_bg=False)\n",
    "\n",
    "\n",
    "            # Non Maximum Supression Outputs\n",
    "            nms_classes, nms_conf, nms_bbs = make_output(pred_classes_1, bb_outputs_1, anchors, grid_sizes)\n",
    "            nms_bbs_as_numpy = torch_corners_to_center(nms_bbs.cpu()).numpy()\n",
    "            torch_gt(axes.flat[5], im, nms_bbs_as_numpy, nms_classes.cpu().numpy(), nms_conf.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = models.resnet18(pretrained=True)\n",
    "layers = list(model_ft.children())[0:-2]\n",
    "layers += [SSDHead()]\n",
    "model = nn.Sequential(*list(layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "anc_grid = 4\n",
    "k = 1\n",
    " \n",
    "anc_offset = 1/(anc_grid*2)\n",
    "anc_x = np.repeat(np.linspace(anc_offset, 1-anc_offset, anc_grid), anc_grid)\n",
    "anc_y = np.tile(np.linspace(anc_offset, 1-anc_offset, anc_grid), anc_grid)\n",
    " \n",
    "anc_ctrs = np.tile(np.stack([anc_x,anc_y], axis=1), (k,1))\n",
    "anc_sizes = np.array([[1/anc_grid,1/anc_grid] for i in range(anc_grid*anc_grid)])\n",
    "anchors = Variable(torch.from_numpy(np.concatenate([anc_ctrs, anc_sizes], axis=1))).float()\n",
    " \n",
    "grid_sizes = Variable(torch.from_numpy(np.array([1/anc_grid]))).unsqueeze(1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = SSDLoss(anchors, grid_sizes, num_classes, imsize)\n",
    "optim_fn = optim.Adam\n",
    "sess = Session(model, criterion, optim_fn, [*[1e-3] * 8, 1e-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test(sess, anchors, grid_sizes, data['valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.747575759887695"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawx, rawy, *_ = next(iter(data['valid'])) \n",
    "sess.step(rawx, rawy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_find(sess, data['train'], start_lr=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.set_lr([*[5e-4 / 2] * 8, 5e-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = Cyclical(len(data['train']) * 4)\n",
    "validator = Validator(data['valid'])\n",
    "schedule = TrainingSchedule(data['train'], [lr_scheduler, validator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler.plot(len(data['train']) * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.train(schedule, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.save('ssd_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.load('ssd_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(sess, anchors, grid_sizes, data['valid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_from_np(arr, requires_grad=True):\n",
    "    return Variable(torch.from_numpy(arr), requires_grad=requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anc_grids = [4,2,1]\n",
    "\n",
    "anc_zooms = [1.]\n",
    "\n",
    "anc_ratios = [(1.,1.)]\n",
    "\n",
    "anchor_scales = [(anz*i,anz*j) for anz in anc_zooms for (i,j) in anc_ratios]\n",
    "\n",
    "# print(anchor_scales)\n",
    "\n",
    "k = len(anchor_scales)\n",
    "\n",
    "anc_offsets = [1/(o*2) for o in anc_grids]\n",
    "\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "anc_x = np.concatenate([np.repeat(np.linspace(ao, 1-ao, ag), ag) for ao, ag in zip(anc_offsets, anc_grids)])\n",
    "\n",
    "# print(anc_x)\n",
    "\n",
    "anc_y = np.concatenate([np.tile(np.linspace(ao, 1-ao, ag), ag) for ao, ag in zip(anc_offsets, anc_grids)])\n",
    "\n",
    "# print(anc_y)\n",
    "\n",
    "anc_ctrs = np.repeat(np.stack([anc_x,anc_y], axis=1), k, axis=0)\n",
    "\n",
    "# print(anc_ctrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "striped_anc_x = np.concatenate([[(i+1)/4-.125 for i in range(4)], [.5, .5, .5, .5]])\n",
    "\n",
    "striped_anc_y = np.concatenate([[.5, .5, .5, .5], [(i+1)/4-.125 for i in range(4)]])\n",
    "\n",
    "striped_anc_ctrs = np.stack([striped_anc_x,striped_anc_y], axis=1)\n",
    "\n",
    "striped_anc_sizes = np.concatenate([[[.25, 1]] * 4, [[1, .25]] * 4])\n",
    "\n",
    "striped_anchors = np.concatenate([striped_anc_ctrs, striped_anc_sizes], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "anc_sizes = np.concatenate([np.array([[o/ag,p/ag] for i in range(ag*ag) for o, p in anchor_scales])\n",
    "               for ag in anc_grids])\n",
    "\n",
    "np_grid_sizes = np.concatenate([np.array([1/ag for i in range(ag*ag) for o, p in anchor_scales])\n",
    "               for ag in anc_grids])\n",
    "\n",
    "np_grid_sizes = np.concatenate([[1] * 8, np_grid_sizes])\n",
    "\n",
    "np_anchors = np.concatenate([striped_anchors, np.concatenate([anc_ctrs, anc_sizes], axis=1)])\n",
    "\n",
    "grid_sizes = var_from_np(np_grid_sizes, requires_grad=False).unsqueeze(1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors = var_from_np(np_anchors, requires_grad=False).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "show_ground_truth(ax, np.ones((3, 224, 224)), anchors.data.cpu().numpy()[0:4] * 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSDOutStripes(nn.Module):\n",
    "    def __init__(self, n_in, k=1):\n",
    "        super().__init__()\n",
    "        self.out_classes_1 = nn.Conv2d(n_in, (num_classes + 1) * k, (4,1), padding=0) # Output for each class + background class\n",
    "        self.out_boxes_1 = nn.Conv2d(n_in, 4*k, (4,1), padding=0) # Output for bounding boxes\n",
    "        \n",
    "        self.out_classes_2 = nn.Conv2d(n_in, (num_classes + 1) * k, (1,4), padding=0) # Output for each class + background class\n",
    "        self.out_boxes_2 = nn.Conv2d(n_in, 4*k, (1,4), padding=0) # Output for bounding boxes\n",
    "        \n",
    "    def forward(self, x):    \n",
    "        oc1 = flatten_conv(self.out_classes_1(x), k)\n",
    "        ob1 = F.tanh(flatten_conv(self.out_boxes_1(x), k))\n",
    "        \n",
    "        oc2 = flatten_conv(self.out_classes_2(x), k)\n",
    "        ob2 = F.tanh(flatten_conv(self.out_boxes_2(x), k))\n",
    "        \n",
    "        return [torch.cat([oc1, oc2], dim=1), torch.cat([ob1, ob2], dim=1)]\n",
    "\n",
    "class SSD_MultiHead(nn.Module):\n",
    "    def __init__(self, k, bias):\n",
    "        super().__init__()\n",
    "        self.drop = nn.Dropout(.4)\n",
    "        self.sconv0 = StdConv(512,256, stride=1, drop_p=.4)\n",
    "        self.sconv1 = StdConv(256,256, drop_p=.4)\n",
    "        self.sconv2 = StdConv(256,256, drop_p=.4)\n",
    "        self.sconv3 = StdConv(256,256, drop_p=.4)\n",
    "        self.out1 = SSDOut(256, k)\n",
    "        self.outStripes = SSDOutStripes(256, k)\n",
    "        self.out2 = SSDOut(256, k)\n",
    "        self.out3 = SSDOut(256, k)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.drop(F.relu(x))\n",
    "        x = self.sconv0(x)\n",
    "        x = self.sconv1(x)\n",
    "        o1c,o1l = self.out1(x)\n",
    "        o11c,o11l = self.outStripes(x) \n",
    "        x = self.sconv2(x)\n",
    "        o2c,o2l = self.out2(x)\n",
    "        x = self.sconv3(x)\n",
    "        o3c,o3l = self.out3(x)\n",
    "        return [torch.cat([o11c,o1c,o2c,o3c], dim=1),\n",
    "                torch.cat([o11l,o1l,o2l,o3l], dim=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = models.resnet18(pretrained=True)\n",
    "layers = list(model_ft.children())[0:-2]\n",
    "layers += [SSD_MultiHead(k, -4.)]\n",
    "model = nn.Sequential(*list(layers))\n",
    "criterion = SSDLoss(anchors, grid_sizes, num_classes, imsize)\n",
    "optim_fn = optim.Adam\n",
    "sess = Session(model, criterion, optim_fn, [*[1e-3] * 8, 1e-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test(sess, anchors, grid_sizes, data['valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_find(sess, data['train'], start_lr=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.set_lr([*[1e-4 / 2] * 8, 1e-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = Cyclical(len(data['train']) * 7)\n",
    "accuracy = JaccardAccuracy(anchors, grid_sizes, imsize)\n",
    "validator = Validator(data['valid'], accuracy)\n",
    "schedule = TrainingSchedule(data['train'], [lr_scheduler, validator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db485edc6e36423db39c2434348cc18c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Validating', max=542), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validaton Loss: 15.342819 Validation Accuracy: 0.004380\n"
     ]
    }
   ],
   "source": [
    "validator.run(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.train(schedule, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.save(\"Resnet18MultiStriped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.load(\"Resnet18MultiStriped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(sess, anchors, grid_sizes, data['valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.train(schedule, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.save(\"Resnet34Multi2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.load(\"Resnet34Multi2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test(sess, anchors, grid_sizes, data['valid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_test_data():\n",
    "    inferences = []\n",
    "\n",
    "    prints = 0\n",
    "\n",
    "    with EvalModel(sess.model):\n",
    "        for x,y,meta in data['test']:\n",
    "            pred_classes, bb_outputs = sess.forward(x)\n",
    "            for idx, file in enumerate(meta['file']):\n",
    "                nms_classes, nms_conf, nms_bbs = make_output(pred_classes[idx], bb_outputs[idx], anchors, grid_sizes)\n",
    "\n",
    "                if prints < 16 and random.random() <= .01:\n",
    "                    fig, ax = plt.subplots(figsize=(12,12))\n",
    "                    ax.set_title(file.split(\"\\\\\")[-1])\n",
    "                    im = denorm(x[idx]).numpy()\n",
    "                    nms_bbs_as_numpy = torch_corners_to_center(nms_bbs.cpu()).numpy()\n",
    "                    torch_gt(ax, im, nms_bbs_as_numpy, nms_classes.cpu().numpy(), nms_conf.cpu().numpy())\n",
    "                    prints += 1\n",
    "\n",
    "                nms_classes, nms_conf, nms_bbs = nms_classes.cpu().numpy(), nms_conf.cpu().numpy(), nms_bbs.cpu().numpy()   \n",
    "\n",
    "                preds = []\n",
    "\n",
    "                for idx, cls, bb in zip(range(5), nms_classes, nms_bbs):\n",
    "                    corners = (bb * 600).clip(0,600).astype(int)\n",
    "                    assert(corners[0] < corners[2] and corners[1] < corners[3])\n",
    "                    preds.append(f'{cls} {\" \".join(corners.astype(str))}')\n",
    "\n",
    "                inferences.append({'filename': file.split(\"\\\\\")[-1], 'prediction': \" \".join(preds)})\n",
    "\n",
    "    df = pd.DataFrame(inferences, columns=['filename', 'prediction'])\n",
    "    df.to_csv(f'Submissions/submission.{time.strftime(\"%Y%m%d-%H%M%S\")}.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inference_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastAI custom",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
