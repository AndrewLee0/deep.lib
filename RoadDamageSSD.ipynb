{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches, patheffects\n",
    "import pandas as pd\n",
    "import json\n",
    "import cv2\n",
    "from PIL import ImageDraw, ImageFont\n",
    "from collections import namedtuple, OrderedDict\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from session import *\n",
    "from LR_Schedule.cos_anneal import CosAnneal\n",
    "from LR_Schedule.cyclical import Cyclical\n",
    "from LR_Schedule.lr_find import lr_find\n",
    "from callbacks import *\n",
    "from validation import *\n",
    "from validation import _AccuracyMeter\n",
    "import Datasets.ImageData as ImageData\n",
    "from Transforms.ImageTransforms import *\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0); torch.backends.cudnn.benchmark=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('C:/fastai/courses/dl2/data/road_damage_dataset')\n",
    "MULTICLASS_CSV_PATH = DATA_PATH/'mc.csv'\n",
    "MULTIBB_CSV_PATH = DATA_PATH/'bb.csv'\n",
    "files, mcs = ImageData.parse_csv_data(MULTICLASS_CSV_PATH)\n",
    "files, mbbs = ImageData.parse_csv_data(MULTIBB_CSV_PATH)\n",
    "mcs = [mc.split(' ') for mc in mcs]\n",
    "classes = [\"bg\"] + sorted(list(set(itertools.chain.from_iterable(mcs))))\n",
    "label2idx = {v:k for k,v in enumerate(classes)}\n",
    "mcs = [[label2idx[c] for c in mc] for mc in mcs]\n",
    "mbbs = [corners_to_center([int(x) for x in mbb.split(' ')]).tolist() for mbb in mbbs]\n",
    "\n",
    "max_len = max([len(mc) for mc in mcs])\n",
    "\n",
    "for mc, mb in zip(mcs, mbbs):\n",
    "    mc += ([-1] * (max_len - len(mc)))\n",
    "    mb += ([0] * (max_len * 4 - len(mb)))\n",
    "    \n",
    "    mc = np.array(mc)\n",
    "    mb = np.array(mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imsize = 224\n",
    "batch_size = 8\n",
    "num_classes = 8\n",
    "\n",
    "files = [DATA_PATH/file for file in files]\n",
    "labels = [md.StructuredLabel([(cat, md.LabelType.CATEGORY, \"CAT\"), (bb, md.LabelType.BOUNDING_BOX, \"BB\")]) for bb, cat in zip(mbbs, mcs)]\n",
    "\n",
    "train_tfms = TransformList([\n",
    "    RandomScale(imsize, 1.1),\n",
    "    RandomCrop(imsize),\n",
    "    RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_tfms = TransformList([\n",
    "    Scale(imsize),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "denorm = transforms.Normalize(\n",
    "    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.255],\n",
    "    std=[1/0.229, 1/0.224, 1/0.255]\n",
    ")\n",
    "\n",
    "i_dict = md.make_partition_indices(len(labels), {'train': .85, 'valid': .15})\n",
    "\n",
    "idx, test_files = ImageData.parse_csv_data(DATA_PATH/'test_data.csv')\n",
    "test_files = [DATA_PATH/file for file in test_files]\n",
    "\n",
    "datasets = {\n",
    "    'train': ImageData.ImageDataset(util.mask(files, i_dict['train']), util.mask(labels, i_dict['train']), train_tfms),\n",
    "    'valid': ImageData.ImageDataset(util.mask(files, i_dict['valid']), util.mask(labels, i_dict['valid']), val_tfms),\n",
    "    'test': ImageData.ImageDataset(test_files, [0] * len(test_files), val_tfms)\n",
    "}  \n",
    "\n",
    "data = md.ModelData(datasets, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(im, figsize=None, ax=None):\n",
    "    if not ax: fig,ax = plt.subplots(figsize=figsize)\n",
    "    ax.imshow(im)\n",
    "    ax.set_xticks(np.linspace(0, 224, 8))\n",
    "    ax.set_yticks(np.linspace(0, 224, 8))\n",
    "    ax.grid()\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "    return ax\n",
    "\n",
    "def draw_outline(o, lw):\n",
    "    o.set_path_effects([patheffects.Stroke(\n",
    "        linewidth=lw, foreground='black'), patheffects.Normal()])\n",
    "\n",
    "def draw_rect(ax, b, color='white'):\n",
    "    patch = ax.add_patch(patches.Rectangle(b[:2], *b[-2:], fill=False, edgecolor=color, lw=2))\n",
    "    draw_outline(patch, 4)\n",
    "\n",
    "def draw_text(ax, xy, txt, sz=14, color='white'):\n",
    "    text = ax.text(*xy, txt,\n",
    "        verticalalignment='top', color=color, fontsize=sz, weight='bold')\n",
    "    draw_outline(text, 1)\n",
    "    \n",
    "def bb_hw(a): return np.array([a[1],a[0],a[3]-a[1]+1,a[2]-a[0]+1])\n",
    "\n",
    "def draw_im(im, ann):\n",
    "    ax = show_img(im, figsize=(16,8))\n",
    "    for b,c in ann:\n",
    "        b = bb_hw(b)\n",
    "        draw_rect(ax, b)\n",
    "        draw_text(ax, b[:2], cats[c], sz=16)\n",
    "\n",
    "def draw_idx(i):\n",
    "    im_a = trn_anno[i]\n",
    "    im = open_image(IMG_PATH/trn_fns[i])\n",
    "    draw_im(im, im_a)\n",
    "    \n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.colors as mcolors\n",
    "from cycler import cycler\n",
    "\n",
    "def get_cmap(N):\n",
    "    color_norm  = mcolors.Normalize(vmin=0, vmax=N-1)\n",
    "    return cmx.ScalarMappable(norm=color_norm, cmap='Set3').to_rgba\n",
    "\n",
    "num_colr = 12\n",
    "cmap = get_cmap(num_colr)\n",
    "colr_list = [cmap(float(x)) for x in range(num_colr)]\n",
    "\n",
    "def show_ground_truth(ax, x, bbox, clas=None, prs=None, thresh=0.3, show_bg=False):\n",
    "    im = np.moveaxis(x, 0, 2)\n",
    "    bb = [center_to_hw(o) for o in bbox.reshape(-1,4)]\n",
    "    if prs is None:  prs  = [None]*len(bb)\n",
    "    if clas is None: clas = [None]*len(bb)\n",
    "    ax = show_img(im.clip(0,1), ax=ax)\n",
    "    for i,(b,c,pr) in enumerate(zip(bb, clas, prs)):\n",
    "        if((b[2]>0) and (pr is None or pr > thresh) and (show_bg or c != 0)):\n",
    "            draw_rect(ax, b, color=colr_list[i%num_colr])\n",
    "            txt = f'{i}: '\n",
    "            if c is not None: txt += classes[c]\n",
    "            if pr is not None: txt += f' {pr:.2f}'\n",
    "            draw_text(ax, b[:2], txt, color=colr_list[i%num_colr])\n",
    "            \n",
    "def torch_gt(ax, ima, bbox, clas, prs=None, thresh=0.25, show_bg=False):\n",
    "    return show_ground_truth(ax, ima, (bbox*imsize),\n",
    "         clas, prs if prs is not None else None, thresh, show_bg=show_bg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StdConv(nn.Module):\n",
    "    def __init__(self, n_in, n_out, stride=2, drop_p=0.1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(n_in, n_out, kernel_size=3, stride=stride, padding=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.batch_norm = nn.BatchNorm2d(n_out)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.batch_norm(self.relu(self.conv(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_conv(x,k=1):\n",
    "    bs,nf,gx,gy = x.size()\n",
    "    x = x.permute(0,2,3,1).contiguous()\n",
    "    return x.view(bs,-1,nf//k)\n",
    "\n",
    "class SSDOut(nn.Module):\n",
    "    def __init__(self, n_in, k=1):\n",
    "        super().__init__()\n",
    "        self.out_classes = nn.Conv2d(n_in, (num_classes + 1) * k, 3, padding=1) # Output for each class + background class\n",
    "        self.out_boxes = nn.Conv2d(n_in, 4*k, 3, padding=1) # Output for bounding boxes\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return [flatten_conv(self.out_classes(x), k), F.tanh(flatten_conv(self.out_boxes(x), k))] \n",
    "\n",
    "class SSDHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.conv_0 = StdConv(512, 256, stride=1)\n",
    "        self.conv_1 = StdConv(256, 256)\n",
    "        self.out = SSDOut(256)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(x))\n",
    "        x = self.conv_0(x)\n",
    "        x = self.conv_1(x)\n",
    "        return self.out(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_embedding(labels, num_classes):\n",
    "    ret = torch.eye(num_classes)[labels.data.cpu().long()]\n",
    "    return ret\n",
    "\n",
    "class Focal_Loss(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, outputs, label):\n",
    "        target = one_hot_embedding(label, self.num_classes + 1) # +1 for background\n",
    "        target = util.to_gpu(Variable(target[:,1:].contiguous())) # Ignore background and send to GPU\n",
    "        pred = outputs[:,1:] # Get the models predicitons (no background)\n",
    "        weight = self.get_weight(pred,target)\n",
    "        return F.binary_cross_entropy_with_logits(pred, target, weight, size_average=False)/self.num_classes\n",
    "    \n",
    "    def get_weight(self, x, t):\n",
    "        alpha, gamma = 0.25, 1\n",
    "        p = x.sigmoid()\n",
    "        pt = p * t + (1-p) * (1-t)\n",
    "        w = alpha * t + (1-alpha) * (1-t)\n",
    "        return w * (1-pt).pow(gamma)\n",
    "\n",
    "loss_f = Focal_Loss(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_center_to_corners(bb): \n",
    "    x1 = bb[:,0] - bb[:,2] / 2\n",
    "    y1 = bb[:,1] - bb[:,3] / 2\n",
    "    x2 = bb[:,0] + bb[:,2] / 2\n",
    "    y2 = bb[:,1] + bb[:,3] / 2\n",
    "    return torch.stack([x1, y1, x2, y2], dim=1)\n",
    "\n",
    "def torch_corners_to_center(bb):\n",
    "    if len(bb.size()) == 0: return bb\n",
    "\n",
    "    center_x = (bb[:,0] + bb[:,2]) / 2\n",
    "    center_y = (bb[:,1] + bb[:,3]) / 2\n",
    "    width = bb[:,2] - bb[:,0]\n",
    "    height = bb[:,3] - bb[:,1]\n",
    "    return torch.stack([center_x, center_y, width, height], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect(box_a, box_b, log=False):\n",
    "    if log: print(\"Intersect\"); print(\"box_a: \", box_a); print(\"box_b: \", box_b)\n",
    "        \n",
    "    corn_a = torch_center_to_corners(box_a)\n",
    "    corn_b = torch_center_to_corners(box_b)\n",
    "    \n",
    "    if log: print(\"corn_a: \", corn_a); print(\"corn_b: \", corn_b)\n",
    "    \n",
    "    max_xy = torch.min(corn_a[:, None, 2:].double(), corn_b[None, :, 2:].double())\n",
    "    min_xy = torch.max(corn_a[:, None, :2].double(), corn_b[None, :, :2].double())\n",
    "    \n",
    "    if log: print(\"max_xy: \", max_xy); print(\"min_xy: \", min_xy)\n",
    "    \n",
    "    inter = torch.clamp((max_xy - min_xy), min=0)\n",
    "    \n",
    "    if log: print(\"inter: \", inter)\n",
    "    \n",
    "    return inter[:, :, 0] * inter[:, :, 1]\n",
    "\n",
    "def box_size(b): return (b[:, 2] * b[:, 3]) # Input [_, _, width, height]\n",
    "\n",
    "def jaccard(box_a, box_b, log=False):\n",
    "    inter = intersect(box_a, box_b, log)\n",
    "    union = box_size(box_a).double().unsqueeze(1) + box_size(box_b).double().unsqueeze(0) - inter\n",
    "    return inter / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Remove padding from labels. \n",
    "Split concatenated bounding boxes into arrays of 4. \n",
    "Divide bounding box values by image size.\n",
    "Labels are padded with -1s so they are the same shape and may be broadcast together in batches.\n",
    "\"\"\"\n",
    "def format_label(bbs, classes, log=False):\n",
    "    if log: print(\"format_label\"); print(\"bbs: \", bbs); print(\"classes: \", classes)\n",
    "    bbs = bbs.view(-1,4)/imsize\n",
    "    keep_idxs = (classes>-1).nonzero().view(-1)\n",
    "    if log: print(\"Output\"); print(\"bbs: \", bbs[keep_idxs]); print(\"classes: \", classes[keep_idxs])\n",
    "    return bbs[keep_idxs], classes[keep_idxs]\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "Maps bounding box outputs to bounding boxes. \n",
    "The model's bounding box outputs are not bounding boxes and instead represent changes to the anchor boxes.  \n",
    "\"\"\"\n",
    "def map_bb_outputs_to_pred_bbs(outputs, anchors, log=False):\n",
    "    if log: print(\"map_bb_outputs_to_pred_bbs\"); print(\"outputs :\", outputs); print(\"anchors :\", anchors)\n",
    "    \n",
    "    # The first two values in the output represent a translation of the anchor box's center.\n",
    "    # Grid size is the width and height of the receptive field\n",
    "    # delta_center is bounded on the range (-grid_size / 2, grid_size / 2); \n",
    "    # that is, the center remains within the original receptive field. \n",
    "    delta_center = outputs[:,:2] / 2 * util.to_gpu(grid_sizes) \n",
    "    \n",
    "    if log: print(\"delta_center :\", delta_center); print(\"grid_sizes :\", grid_sizes)\n",
    "    \n",
    "    # The last two values in the output represent the width and height of the bounding box.\n",
    "    # These values are interpreted as a precentage of the original anchor box's width and height.\n",
    "    # percent_sizes is on the range (.5, 1.5). We add 1 since actn_bbs is on the range (-1, 1)\n",
    "    percent_sizes = outputs[:,2:] / 2 + 1 \n",
    "    \n",
    "    if log: print(\"percent_sizes :\", percent_sizes);\n",
    "    \n",
    "    actn_centers = delta_center + util.to_gpu(anchors)[:,:2]  # Calculate predicted center_x and center_y  \n",
    "    actn_wh = percent_sizes * util.to_gpu(anchors)[:,2:]      # Calculate predicted width and height\n",
    "    \n",
    "    if log: print(\"returns :\", torch.cat([actn_centers, actn_wh], dim=1));\n",
    "    \n",
    "    return torch.cat([actn_centers, actn_wh], dim=1)\n",
    "\n",
    "\n",
    "def map_label_to_ground_truth(raw_label_bbs, raw_label_classes, anchors, log=False):\n",
    "    label_bbs, label_classes = format_label(raw_label_bbs, raw_label_classes)\n",
    "        \n",
    "    if log: print(\"map_label_to_ground_truth\"); print(\"label_bbs: \", label_bbs); print(\"label_classes: \", label_classes)\n",
    "    \n",
    "    overlaps = jaccard(label_bbs, anchors)\n",
    "    \n",
    "    if log: print(\"overlaps: \", overlaps)\n",
    "    \n",
    "    prior_overlap, prior_idx = overlaps.max(1)\n",
    "    \n",
    "    if log: print(\"prior_overlap: \", prior_overlap); print(\"prior_idx: \", prior_idx)\n",
    "    \n",
    "    gt_overlap, gt_idx = overlaps.max(0)\n",
    "    \n",
    "    if log: print(\"gt_overlap: \", gt_overlap); print(\"gt_idx: \", gt_idx)\n",
    "    \n",
    "    gt_overlap[prior_idx] = 1.99\n",
    "    \n",
    "    for i,o in enumerate(prior_idx): gt_idx[o] = i\n",
    "        \n",
    "    if log: print(\"gt_overlap: \", gt_overlap); print(\"gt_idx: \", gt_idx)\n",
    "        \n",
    "    gt_classes = label_classes[gt_idx]\n",
    "    \n",
    "    if log: print(\"gt_classes: \", gt_classes)\n",
    "    \n",
    "    matches = gt_overlap > 0.4\n",
    "    \n",
    "    if log: print(\"matches: \", matches)\n",
    "    \n",
    "    matching_idxs = torch.nonzero(matches)[:,0]\n",
    "    \n",
    "    if log: print(\"matching_idxs: \", matching_idxs)\n",
    "    \n",
    "    gt_classes[matches != 1] = 0\n",
    "    \n",
    "    gt_bbs = label_bbs[gt_idx]\n",
    "    \n",
    "    if log: print(\"gt_classes: \", gt_classes[matching_idxs]); print(\"gt_bbs: \", gt_bbs[matching_idxs]);\n",
    "        \n",
    "    return util.to_gpu(gt_bbs), gt_classes, util.to_gpu(matching_idxs)\n",
    "\n",
    "\n",
    "\"\"\" ssd loss for a single example \"\"\"\n",
    "def ssd_1_loss(pred_classes, bb_outputs, label_classes, label_bbs):      \n",
    "    gt_bbs, gt_classes, matching_idxs = map_label_to_ground_truth(label_bbs, label_classes, anchors)\n",
    "    \n",
    "    pred_bbs = map_bb_outputs_to_pred_bbs(bb_outputs, anchors)\n",
    "    \n",
    "    loc_loss = ((pred_bbs[matching_idxs].float() - gt_bbs[matching_idxs].float()).abs()).mean()\n",
    "    \n",
    "    clas_loss = loss_f(pred_classes, gt_classes)\n",
    "    \n",
    "    return loc_loss, clas_loss\n",
    "\n",
    "\n",
    "\"\"\" ssd loss for a batch \"\"\"\n",
    "def ssd_loss(preds, target, log=False):\n",
    "    total_location_loss = 0.\n",
    "    total_class_loss = 0.\n",
    "    \n",
    "    for pred_clas, pred_bb, label_clas, label_bb in zip(*preds, target[\"CAT\"], target[\"BB\"]):\n",
    "        \n",
    "        loc_loss, clas_loss = ssd_1_loss(pred_clas, pred_bb, \n",
    "                                         label_clas, label_bb)\n",
    "        \n",
    "        total_location_loss += loc_loss\n",
    "        total_class_loss += clas_loss\n",
    "        \n",
    "    if log: print(f'location: {total_location_loss.data[0]}, class: {total_class_loss.data[0]}')\n",
    "        \n",
    "    return total_location_loss + total_class_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_maximum_supression(boxes, scores, overlap=0.5, top_k=100):\n",
    "    keep = scores.new(scores.size(0)).zero_().long()\n",
    "    if boxes.numel() == 0: return keep\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    "    area = torch.mul(x2 - x1, y2 - y1)\n",
    "    v, idx = scores.sort(0)  # sort in ascending order\n",
    "    idx = idx[-top_k:]  # indices of the top-k largest vals\n",
    "    xx1 = boxes.new()\n",
    "    yy1 = boxes.new()\n",
    "    xx2 = boxes.new()\n",
    "    yy2 = boxes.new()\n",
    "    w = boxes.new()\n",
    "    h = boxes.new()\n",
    "\n",
    "    count = 0\n",
    "    while idx.numel() > 0:\n",
    "        i = idx[-1]  # index of current largest val\n",
    "        keep[count] = i\n",
    "        count += 1\n",
    "        if idx.size(0) == 1: break\n",
    "        idx = idx[:-1]  # remove kept element from view\n",
    "        # load bboxes of next highest vals\n",
    "        torch.index_select(x1, 0, idx, out=xx1)\n",
    "        torch.index_select(y1, 0, idx, out=yy1)\n",
    "        torch.index_select(x2, 0, idx, out=xx2)\n",
    "        torch.index_select(y2, 0, idx, out=yy2)\n",
    "        # store element-wise max with next highest score\n",
    "        xx1 = torch.clamp(xx1, min=x1[i])\n",
    "        yy1 = torch.clamp(yy1, min=y1[i])\n",
    "        xx2 = torch.clamp(xx2, max=x2[i])\n",
    "        yy2 = torch.clamp(yy2, max=y2[i])\n",
    "        w.resize_as_(xx2)\n",
    "        h.resize_as_(yy2)\n",
    "        w = xx2 - xx1\n",
    "        h = yy2 - yy1\n",
    "        # check sizes of xx1 and xx2.. after each iteration\n",
    "        w = torch.clamp(w, min=0.0)\n",
    "        h = torch.clamp(h, min=0.0)\n",
    "        inter = w*h\n",
    "        # IoU = i / (area(a) + area(b) - i)\n",
    "        rem_areas = torch.index_select(area, 0, idx)  # load remaining areas)\n",
    "        union = (rem_areas - inter) + area[i]\n",
    "        IoU = inter/union  # store result in iou\n",
    "        # keep only elements with an IoU <= overlap\n",
    "        idx = idx[IoU.le(overlap)]\n",
    "    return keep, count\n",
    "\n",
    "def make_output(pred_classes, bb_outputs, log=False):\n",
    "    pred_bbs = torch_center_to_corners(map_bb_outputs_to_pred_bbs(bb_outputs, anchors))\n",
    "    \n",
    "    if log: print(\"pred_bbs: \", pred_bbs)\n",
    "        \n",
    "    class_preds, clas_idxs = pred_classes.max(1)\n",
    "    \n",
    "    class_preds = class_preds.sigmoid()\n",
    "    \n",
    "    if log: print(\"class_preds: \", class_preds)\n",
    "    \n",
    "    conf_scores = pred_classes.sigmoid().t().data   \n",
    "    \n",
    "    if log: print(\"conf_scores: \", conf_scores)\n",
    "    \n",
    "    out1,out2,cc = [],[],[]\n",
    "    for class_idx in range(1, len(conf_scores)):\n",
    "        \n",
    "        if log: print(\"class_idx: \", class_idx)\n",
    "        \n",
    "        c_mask = conf_scores[class_idx] > 0.2\n",
    "        \n",
    "        if log: print(\"c_mask: \", c_mask)\n",
    "        \n",
    "        if c_mask.sum() == 0: continue\n",
    "            \n",
    "        scores = conf_scores[class_idx][c_mask]\n",
    "        \n",
    "        if log: print(\"scores: \", scores)\n",
    "            \n",
    "        l_mask = c_mask.unsqueeze(1).expand_as(pred_bbs)\n",
    "        \n",
    "        if log: print(\"l_mask: \", l_mask)\n",
    "        \n",
    "        boxes = pred_bbs[l_mask].view(-1, 4)\n",
    "        \n",
    "        if log: print(\"boxes: \", boxes)\n",
    "        \n",
    "        ids, count = non_maximum_supression(boxes.data, scores, 0.4, 5)\n",
    "        \n",
    "        if log: print(\"ids: \", ids, \" count: \", count)\n",
    "        \n",
    "        ids = ids[:count]\n",
    "        \n",
    "        if log: print(\"ids: \", ids)\n",
    "             \n",
    "        out1.append(scores[ids])\n",
    "        \n",
    "        if log: print(\"scores: \", scores[ids])\n",
    "        \n",
    "        out2.append(boxes.data[ids])\n",
    "        \n",
    "        if log: print(\"boxes: \", boxes[ids])\n",
    "        \n",
    "        cc.append([class_idx]*count)\n",
    "        \n",
    "        if log: print(\"classes: \", [class_idx]*count)\n",
    "    \n",
    "    if(len(cc) == 0): return torch.Tensor(), torch.Tensor(), torch.Tensor()\n",
    "    \n",
    "    cc = torch.from_numpy(np.concatenate(cc))\n",
    "    out1 = torch.cat(out1)\n",
    "    out2 = torch.cat(out2)\n",
    "    \n",
    "    return cc, out1, out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(sess, anchors, data):\n",
    "    with EvalModel(sess.model):\n",
    "        rawx, rawy, *_ = next(iter(data))\n",
    "        pred_classes, bb_outputs = sess.forward(rawx)\n",
    "        prints = {key: val.numpy() for key, val in rawy.items()}\n",
    "        y = {key: Variable(value) for key, value in rawy.items()}\n",
    "\n",
    "        for i, x in enumerate(rawx[0:16]):\n",
    "            im = denorm(rawx[i]).numpy()\n",
    "\n",
    "            pred_classes_1, bb_outputs_1 = pred_classes[i], bb_outputs[i]\n",
    "            label_bbs, label_classes = y['BB'][i], y['CAT'][i]\n",
    "\n",
    "            fig, axes = plt.subplots(3, 2, figsize=(18, 18))\n",
    "\n",
    "\n",
    "            # Ground Truth\n",
    "            show_ground_truth(axes.flat[0], im, prints['BB'][i], prints['CAT'][i])\n",
    "\n",
    "\n",
    "            # Anchorbox Assignments \n",
    "            gt_bbs, gt_classes, *_ = map_label_to_ground_truth(label_bbs, label_classes, anchors)\n",
    "            torch_gt(axes.flat[1], im, anchors.cpu().data.numpy(), gt_classes.cpu().data.numpy(), show_bg=False)\n",
    "\n",
    "\n",
    "            # Predicted classes per anchorbox\n",
    "            torch_gt(axes.flat[2], im, \n",
    "                     anchors.cpu().data.numpy(), \n",
    "                     pred_classes_1.max(1)[1].data, \n",
    "                     pred_classes_1.max(1)[0].sigmoid().data, \n",
    "                     show_bg=True)\n",
    "\n",
    "\n",
    "            # Predicted classes per anchorbox. No background\n",
    "            torch_gt(axes.flat[3], im, \n",
    "                     anchors.cpu().data.numpy(), \n",
    "                     pred_classes_1[:,1:].max(1)[1].data + 1, \n",
    "                     pred_classes_1[:,1:].max(1)[0].sigmoid().data, \n",
    "                     thresh=0.15,\n",
    "                     show_bg=False)\n",
    "\n",
    "\n",
    "            # Predicted class and bounding box\n",
    "            a_ic = map_bb_outputs_to_pred_bbs(bb_outputs_1, anchors)\n",
    "            torch_gt(axes.flat[4], im, \n",
    "                     a_ic.cpu().data.numpy(), \n",
    "                     pred_classes_1[:,1:].max(1)[1].data + 1, \n",
    "                     pred_classes_1[:,1:].max(1)[0].sigmoid().data, \n",
    "                     thresh=0.15, \n",
    "                     show_bg=False)\n",
    "\n",
    "\n",
    "            # Non Maximum Supression Outputs\n",
    "            nms_classes, nms_conf, nms_bbs = make_output(pred_classes_1, bb_outputs_1)\n",
    "            nms_bbs_as_numpy = torch_corners_to_center(nms_bbs.cpu()).numpy()\n",
    "            torch_gt(axes.flat[5], im, nms_bbs_as_numpy, nms_classes.cpu().numpy(), nms_conf.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JaccardAccuracy(_AccuracyMeter):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.num_true_positives = 0\n",
    "        self.num_false_positives = 0\n",
    "        self.num_false_negatives = 0\n",
    "        \n",
    "    def update(self, output, label, log=False):\n",
    "        pred_classes, bb_outputs = output\n",
    "        for i, x in enumerate(label['CAT']): \n",
    "            if log: print(\"############ Next Example ###############\")\n",
    "                    \n",
    "            pred_classes_i, bb_outputs_i = pred_classes[i], bb_outputs[i]   \n",
    "              \n",
    "            label_bbs, label_classes = format_label(label['BB'][i], label['CAT'][i])    \n",
    "            label_bbs, label_classes = label_bbs.data.cpu(), label_classes.data.cpu()\n",
    "            nms_classes, nms_conf, nms_bbs = make_output(pred_classes_i, bb_outputs_i)\n",
    "            nms_classes, nms_conf, nms_bbs = nms_classes.cpu(), nms_conf.cpu(), torch_corners_to_center(nms_bbs.cpu())\n",
    "            \n",
    "            if log: print(\"Ground Truth Classes: \", label_classes); print(\"Ground Truth Bounding Boxes: \", label_bbs)\n",
    "            \n",
    "            if(len(nms_classes.size()) == 0): \n",
    "                if log: print(\"No Predictions\")\n",
    "                if log: print(f\"Adding {label_classes.size()[0]} false negative(s)\")\n",
    "                self.num_false_negatives += label_classes.size()[0]\n",
    "                continue   \n",
    "            \n",
    "            if log: print(\"Predicted Classes: \", nms_classes); print(\"Predicted Bounding Boxes: \", nms_bbs)\n",
    "                \n",
    "            pred_hits = torch.zeros(nms_classes.size()[0])\n",
    "            label_hits = torch.zeros(len(label_classes))\n",
    "                \n",
    "            overlaps = jaccard(label_bbs, nms_bbs)\n",
    "            \n",
    "            for idx, cls, overlap in zip(range(len(label_classes)), label_classes, overlaps):              \n",
    "                if log: print(\"------------ Next Label In Example -------------\")\n",
    "                if log: print(\"Ground Truth Class: \", cls); print(\"Overlaps For Class: \", overlap)\n",
    "                \n",
    "                matches = (overlap >= .5) * (nms_classes == cls)       \n",
    "                \n",
    "                if(matches.sum() > 0):\n",
    "                    label_hits[idx] = 1\n",
    "                                         \n",
    "                if log: print(\"Predicted Bounding Boxes with the Correct Class Label and an Overlap >= .5: \", matches)\n",
    "                    \n",
    "                pred_hits[matches] = 1\n",
    "            \n",
    "            if log: print(\"------------ Results -------------\")\n",
    "                \n",
    "            if log: print(\"Ground Truth Hits: \", label_hits)           \n",
    "            if log: print(\"Prediction Hits: \", pred_hits) \n",
    "                \n",
    "            self.num_true_positives += label_hits.sum()\n",
    "            \n",
    "            # self.num_correct_positives += pred_hits.sum()\n",
    "            \n",
    "            self.num_false_negatives += (label_hits != 1).sum()\n",
    "            self.num_false_positives += (pred_hits != 1).sum()\n",
    "            \n",
    "            if log: print(f\"{label_hits.sum()} true positives. {(pred_hits != 1).sum()} false positives. {(label_hits != 1).sum()} false negatives.\")            \n",
    "                                     \n",
    "        \n",
    "    def accuracy(self):\n",
    "        precision = self.num_true_positives / (self.num_true_positives + self.num_false_positives)\n",
    "        recall = self.num_true_positives / (self.num_true_positives + self.num_false_negatives) \n",
    "        return 2 * (precision * recall / (precision + recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = models.resnet18(pretrained=True)\n",
    "layers = list(model_ft.children())[0:-2]\n",
    "layers += [SSDHead()]\n",
    "model = nn.Sequential(*list(layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anc_grid = 4\n",
    "k = 1\n",
    " \n",
    "anc_offset = 1/(anc_grid*2)\n",
    "anc_x = np.repeat(np.linspace(anc_offset, 1-anc_offset, anc_grid), anc_grid)\n",
    "anc_y = np.tile(np.linspace(anc_offset, 1-anc_offset, anc_grid), anc_grid)\n",
    " \n",
    "anc_ctrs = np.tile(np.stack([anc_x,anc_y], axis=1), (k,1))\n",
    "anc_sizes = np.array([[1/anc_grid,1/anc_grid] for i in range(anc_grid*anc_grid)])\n",
    "anchors = Variable(torch.from_numpy(np.concatenate([anc_ctrs, anc_sizes], axis=1))).float()\n",
    " \n",
    "grid_sizes = Variable(torch.from_numpy(np.array([1/anc_grid]))).unsqueeze(1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = ssd_loss\n",
    "optim_fn = optim.Adam\n",
    "sess = Session(model, criterion, optim_fn, [*[1e-3] * 8, 1e-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test(sess, anchors, data['valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawx, rawy, *_ = next(iter(data['valid'])) \n",
    "sess.step(rawx, rawy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_find(sess, data['train'], start_lr=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.set_lr([*[5e-4 / 2] * 8, 5e-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = Cyclical(len(data['train']) * 4)\n",
    "validator = Validator(data['valid'])\n",
    "schedule = TrainingSchedule(data['train'], [lr_scheduler, validator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler.plot(len(data['train']) * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.train(schedule, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.save('ssd_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.load('ssd_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(sess, anchors, data['valid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_from_np(arr, requires_grad=True):\n",
    "    return Variable(torch.from_numpy(arr), requires_grad=requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anc_grids = [4,2,1]\n",
    "\n",
    "anc_zooms = [1.]\n",
    "\n",
    "anc_ratios = [(1.,1.)]\n",
    "\n",
    "anchor_scales = [(anz*i,anz*j) for anz in anc_zooms for (i,j) in anc_ratios]\n",
    "\n",
    "# print(anchor_scales)\n",
    "\n",
    "k = len(anchor_scales)\n",
    "\n",
    "anc_offsets = [1/(o*2) for o in anc_grids]\n",
    "\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "anc_x = np.concatenate([np.repeat(np.linspace(ao, 1-ao, ag), ag) for ao, ag in zip(anc_offsets, anc_grids)])\n",
    "\n",
    "# print(anc_x)\n",
    "\n",
    "anc_y = np.concatenate([np.tile(np.linspace(ao, 1-ao, ag), ag) for ao, ag in zip(anc_offsets, anc_grids)])\n",
    "\n",
    "# print(anc_y)\n",
    "\n",
    "anc_ctrs = np.repeat(np.stack([anc_x,anc_y], axis=1), k, axis=0)\n",
    "\n",
    "# print(anc_ctrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.125, 0.5  , 0.25 , 1.   ],\n",
       "       [0.375, 0.5  , 0.25 , 1.   ],\n",
       "       [0.625, 0.5  , 0.25 , 1.   ],\n",
       "       [0.875, 0.5  , 0.25 , 1.   ],\n",
       "       [0.5  , 0.125, 1.   , 0.25 ],\n",
       "       [0.5  , 0.375, 1.   , 0.25 ],\n",
       "       [0.5  , 0.625, 1.   , 0.25 ],\n",
       "       [0.5  , 0.875, 1.   , 0.25 ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "striped_anc_x = np.concatenate([[(i+1)/4-.125 for i in range(4)], [.5, .5, .5, .5]])\n",
    "\n",
    "striped_anc_y = np.concatenate([[.5, .5, .5, .5], [(i+1)/4-.125 for i in range(4)]])\n",
    "\n",
    "striped_anc_ctrs = np.stack([striped_anc_x,striped_anc_y], axis=1)\n",
    "\n",
    "striped_anc_sizes = np.concatenate([[[.25, 1]] * 4, [[1, .25]] * 4])\n",
    "\n",
    "striped_anchors = np.concatenate([striped_anc_ctrs, striped_anc_sizes], axis=1)\n",
    "striped_anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.0000\n",
       " 1.0000\n",
       " 1.0000\n",
       " 1.0000\n",
       " 1.0000\n",
       " 1.0000\n",
       " 1.0000\n",
       " 1.0000\n",
       " 0.2500\n",
       " 0.2500\n",
       " 0.2500\n",
       " 0.2500\n",
       " 0.2500\n",
       " 0.2500\n",
       " 0.2500\n",
       " 0.2500\n",
       " 0.2500\n",
       " 0.2500\n",
       " 0.2500\n",
       " 0.2500\n",
       " 0.2500\n",
       " 0.2500\n",
       " 0.2500\n",
       " 0.2500\n",
       " 0.5000\n",
       " 0.5000\n",
       " 0.5000\n",
       " 0.5000\n",
       " 1.0000\n",
       "[torch.FloatTensor of size 29x1]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anc_sizes = np.concatenate([np.array([[o/ag,p/ag] for i in range(ag*ag) for o, p in anchor_scales])\n",
    "               for ag in anc_grids])\n",
    "\n",
    "np_grid_sizes = np.concatenate([np.array([1/ag for i in range(ag*ag) for o, p in anchor_scales])\n",
    "               for ag in anc_grids])\n",
    "\n",
    "np_grid_sizes = np.concatenate([[1] * 8, np_grid_sizes])\n",
    "\n",
    "np_anchors = np.concatenate([striped_anchors, np.concatenate([anc_ctrs, anc_sizes], axis=1)])\n",
    "\n",
    "grid_sizes = var_from_np(np_grid_sizes, requires_grad=False).unsqueeze(1).float()\n",
    "grid_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.1250  0.5000  0.2500  1.0000\n",
       " 0.3750  0.5000  0.2500  1.0000\n",
       " 0.6250  0.5000  0.2500  1.0000\n",
       " 0.8750  0.5000  0.2500  1.0000\n",
       " 0.5000  0.1250  1.0000  0.2500\n",
       " 0.5000  0.3750  1.0000  0.2500\n",
       " 0.5000  0.6250  1.0000  0.2500\n",
       " 0.5000  0.8750  1.0000  0.2500\n",
       " 0.1250  0.1250  0.2500  0.2500\n",
       " 0.1250  0.3750  0.2500  0.2500\n",
       " 0.1250  0.6250  0.2500  0.2500\n",
       " 0.1250  0.8750  0.2500  0.2500\n",
       " 0.3750  0.1250  0.2500  0.2500\n",
       " 0.3750  0.3750  0.2500  0.2500\n",
       " 0.3750  0.6250  0.2500  0.2500\n",
       " 0.3750  0.8750  0.2500  0.2500\n",
       " 0.6250  0.1250  0.2500  0.2500\n",
       " 0.6250  0.3750  0.2500  0.2500\n",
       " 0.6250  0.6250  0.2500  0.2500\n",
       " 0.6250  0.8750  0.2500  0.2500\n",
       " 0.8750  0.1250  0.2500  0.2500\n",
       " 0.8750  0.3750  0.2500  0.2500\n",
       " 0.8750  0.6250  0.2500  0.2500\n",
       " 0.8750  0.8750  0.2500  0.2500\n",
       " 0.2500  0.2500  0.5000  0.5000\n",
       " 0.2500  0.7500  0.5000  0.5000\n",
       " 0.7500  0.2500  0.5000  0.5000\n",
       " 0.7500  0.7500  0.5000  0.5000\n",
       " 0.5000  0.5000  1.0000  1.0000\n",
       "[torch.FloatTensor of size 29x4]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchors = var_from_np(np_anchors, requires_grad=False).float()\n",
    "anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "show_ground_truth(ax, np.ones((3, 224, 224)), anchors.data.cpu().numpy()[0:4] * 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSDOutStripes(nn.Module):\n",
    "    def __init__(self, n_in, k=1):\n",
    "        super().__init__()\n",
    "        self.out_classes_1 = nn.Conv2d(n_in, (num_classes + 1) * k, (4,1), padding=0) # Output for each class + background class\n",
    "        self.out_boxes_1 = nn.Conv2d(n_in, 4*k, (4,1), padding=0) # Output for bounding boxes\n",
    "        \n",
    "        self.out_classes_2 = nn.Conv2d(n_in, (num_classes + 1) * k, (1,4), padding=0) # Output for each class + background class\n",
    "        self.out_boxes_2 = nn.Conv2d(n_in, 4*k, (1,4), padding=0) # Output for bounding boxes\n",
    "        \n",
    "    def forward(self, x):    \n",
    "        oc1 = flatten_conv(self.out_classes_1(x), k)\n",
    "        ob1 = F.tanh(flatten_conv(self.out_boxes_1(x), k))\n",
    "        \n",
    "        oc2 = flatten_conv(self.out_classes_2(x), k)\n",
    "        ob2 = F.tanh(flatten_conv(self.out_boxes_2(x), k))\n",
    "        \n",
    "        return [torch.cat([oc1, oc2], dim=1), torch.cat([ob1, ob2], dim=1)]\n",
    "\n",
    "class SSD_MultiHead(nn.Module):\n",
    "    def __init__(self, k, bias):\n",
    "        super().__init__()\n",
    "        self.drop = nn.Dropout(.4)\n",
    "        self.sconv0 = StdConv(512,256, stride=1, drop_p=.4)\n",
    "        self.sconv1 = StdConv(256,256, drop_p=.4)\n",
    "        self.sconv2 = StdConv(256,256, drop_p=.4)\n",
    "        self.sconv3 = StdConv(256,256, drop_p=.4)\n",
    "        self.out1 = SSDOut(256, k)\n",
    "        self.outStripes = SSDOutStripes(256, k)\n",
    "        self.out2 = SSDOut(256, k)\n",
    "        self.out3 = SSDOut(256, k)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.drop(F.relu(x))\n",
    "        x = self.sconv0(x)\n",
    "        x = self.sconv1(x)\n",
    "        o1c,o1l = self.out1(x)\n",
    "        o11c,o11l = self.outStripes(x) \n",
    "        x = self.sconv2(x)\n",
    "        o2c,o2l = self.out2(x)\n",
    "        x = self.sconv3(x)\n",
    "        o3c,o3l = self.out3(x)\n",
    "        return [torch.cat([o11c,o1c,o2c,o3c], dim=1),\n",
    "                torch.cat([o11l,o1l,o2l,o3l], dim=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = models.resnet18(pretrained=True)\n",
    "layers = list(model_ft.children())[0:-2]\n",
    "layers += [SSD_MultiHead(k, -4.)]\n",
    "model = nn.Sequential(*list(layers))\n",
    "criterion = ssd_loss\n",
    "optim_fn = optim.Adam\n",
    "sess = Session(model, criterion, optim_fn, [*[1e-3] * 8, 1e-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawx,rawy,*_ = next(iter(data['valid']))\n",
    "y = {key: Variable(util.to_gpu(value)) for key, value in rawy.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = sess.forward(rawx)\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "accuracy = JaccardAccuracy()\n",
    "accuracy.update(batch, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test(sess, anchors, data['valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_find(sess, data['train'], start_lr=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.set_lr([*[1e-4 / 2] * 8, 1e-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = CosAnneal(len(data['train']), T_mult=2)\n",
    "validator = Validator(data['valid'])\n",
    "schedule = TrainingSchedule(data['train'], [lr_scheduler, validator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de98029f177a4717a5790c6537c92d9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epochs', max=7), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Steps', max=769), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b27f43e1344c63a56c247c83a5a35b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Validating', max=136), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 4.054085  Validaton Loss: 3.813722 Validation Accuracy: 0.000000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Steps', max=769), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be735dc12cc543779b11919273aec423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Validating', max=136), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.408379  Validaton Loss: 2.206338 Validation Accuracy: 0.000000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Steps', max=769), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "865328df6afa44a7a4d17506acc8cbd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Validating', max=136), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.243489  Validaton Loss: 2.075012 Validation Accuracy: 0.000000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffc8c742a1e04483abfb896ecf14ad5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Steps', max=769), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "Traceback (most recent call last):\n  File \"C:\\deep.lib\\Datasets\\ImageData.py\", line 38, in open_image\n    im = cv2.imread(str(fn), flags).astype(np.float32)/255\nMemoryError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\Drake\\AppData\\Local\\conda\\conda\\envs\\fastai\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 42, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"C:\\Users\\Drake\\AppData\\Local\\conda\\conda\\envs\\fastai\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 42, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"C:\\deep.lib\\Datasets\\ImageData.py\", line 74, in __getitem__\n    x, y = self.transform(open_image(file), label)\n  File \"C:\\deep.lib\\Datasets\\ImageData.py\", line 42, in open_image\n    raise OSError('Error handling image at: {}'.format(fn)) from e\nOSError: Error handling image at: C:\\fastai\\courses\\dl2\\data\\road_damage_dataset\\Adachi\\JPEGImages\\train_Adachi_00672.jpg\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-6f700475a9db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\deep.lib\\session.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, schedule, epochs)\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschedule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mTrainModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\deep.lib\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, schedule, epochs)\u001b[0m\n\u001b[0;32m    134\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mschedule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Steps\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mschedule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\fastai\\lib\\site-packages\\tqdm\\_tqdm_notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m                 \u001b[1;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\fastai\\lib\\site-packages\\tqdm\\_tqdm.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    929\u001b[0m \"\"\", fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 931\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    932\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    933\u001b[0m                 \u001b[1;31m# Update and possibly print the progressbar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\fastai\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    208\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_next_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[0mnext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[1;31m# Python 2 compatibility\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\fastai\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_process_next_batch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_put_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Traceback (most recent call last):\n  File \"C:\\deep.lib\\Datasets\\ImageData.py\", line 38, in open_image\n    im = cv2.imread(str(fn), flags).astype(np.float32)/255\nMemoryError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\Drake\\AppData\\Local\\conda\\conda\\envs\\fastai\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 42, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"C:\\Users\\Drake\\AppData\\Local\\conda\\conda\\envs\\fastai\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 42, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"C:\\deep.lib\\Datasets\\ImageData.py\", line 74, in __getitem__\n    x, y = self.transform(open_image(file), label)\n  File \"C:\\deep.lib\\Datasets\\ImageData.py\", line 42, in open_image\n    raise OSError('Error handling image at: {}'.format(fn)) from e\nOSError: Error handling image at: C:\\fastai\\courses\\dl2\\data\\road_damage_dataset\\Adachi\\JPEGImages\\train_Adachi_00672.jpg\n"
     ]
    }
   ],
   "source": [
    "sess.train(schedule, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.save(\"Resnet18MultiStriped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.load(\"Resnet18MultiStriped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(sess, anchors, data['valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.train(schedule, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.save(\"Resnet34Multi2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.load(\"Resnet34Multi2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test(sess, anchors, data['valid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_test_data():\n",
    "    inferences = []\n",
    "\n",
    "    prints = 0\n",
    "\n",
    "    with EvalModel(sess.model):\n",
    "        for x,y,meta in data['test']:\n",
    "            pred_classes, bb_outputs = sess.forward(x)\n",
    "            for idx, file in enumerate(meta['file']):\n",
    "                nms_classes, nms_conf, nms_bbs = make_output(pred_classes[idx], bb_outputs[idx])\n",
    "\n",
    "                if prints < 16 and random.random() <= .01:\n",
    "                    fig, ax = plt.subplots(figsize=(12,12))\n",
    "                    ax.set_title(file.split(\"\\\\\")[-1])\n",
    "                    im = denorm(x[idx]).numpy()\n",
    "                    nms_bbs_as_numpy = torch_corners_to_center(nms_bbs.cpu()).numpy()\n",
    "                    torch_gt(ax, im, nms_bbs_as_numpy, nms_classes.cpu().numpy(), nms_conf.cpu().numpy())\n",
    "                    prints += 1\n",
    "\n",
    "                nms_classes, nms_conf, nms_bbs = nms_classes.cpu().numpy(), nms_conf.cpu().numpy(), nms_bbs.cpu().numpy()   \n",
    "\n",
    "                preds = []\n",
    "\n",
    "                for idx, cls, bb in zip(range(5), nms_classes, nms_bbs):\n",
    "                    corners = (bb * 600).clip(0,600).astype(int)\n",
    "                    assert(corners[0] < corners[2] and corners[1] < corners[3])\n",
    "                    preds.append(f'{cls} {\" \".join(corners.astype(str))}')\n",
    "\n",
    "                inferences.append({'filename': file.split(\"\\\\\")[-1], 'prediction': \" \".join(preds)})\n",
    "\n",
    "    df = pd.DataFrame(inferences, columns=['filename', 'prediction'])\n",
    "    df.to_csv(f'Submissions/submission.{time.strftime(\"%Y%m%d-%H%M%S\")}.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inference_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastAI custom",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
